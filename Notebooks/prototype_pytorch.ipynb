{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch.optim import adam\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sequence of transformations to be applied on each image\n",
    "transform = v2.Compose([\n",
    "    v2.Resize((224,224)),  #Input images are 224 x 224\n",
    "    v2.PILToTensor(),      #Convert the image to a pytorch tensor\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, data_transform):\n",
    "        self.dataset = pd.read_csv(\"../Dataset/captions.txt\")\n",
    "        self.transform = data_transform\n",
    "        self.vocab = {\"<START>\",\"<END>\", \"<PAD>\"}\n",
    "        self.max_line = 0\n",
    "        self.itos = dict()\n",
    "        self.stoi = dict()\n",
    "        self.__load_vocab()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __load_vocab(self):\n",
    "        for i in range(len(self.dataset)):\n",
    "            line = self.dataset.iloc[i]['caption']\n",
    "            self.max_line = max(self.max_line, len(line.split()))\n",
    "            for word in line.split():\n",
    "                self.vocab.add(word.lower())\n",
    "        self.vocab = list(self.vocab)\n",
    "        self.max_line += 2\n",
    "        for i in range(len(self.vocab)):\n",
    "            self.itos[i] = self.vocab[i]\n",
    "            self.stoi[self.vocab[i]] = i\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.dataset.iloc[index]['image']\n",
    "        img = Image.open('../Dataset/Images/'+img_path)\n",
    "        img = self.transform(img)\n",
    "        caption = self.dataset.iloc[index]['caption']\n",
    "        caption = caption.split()\n",
    "        caption = [self.vocab.index(word.lower()) for word in caption]\n",
    "        caption = [self.vocab.index(\"<START>\")] + caption + [self.vocab.index(\"<END>\")]\n",
    "    \n",
    "        if (len(caption) < self.max_line):\n",
    "            caption = caption + [self.vocab.index(\"<PAD>\")] * (self.max_line - len(caption))\n",
    "        \n",
    "        caption = torch.tensor(caption)\n",
    "        return img, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FlickrDataset(transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if  torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(inp, title=None):\n",
    "    \"\"\"Image for Tensor\"\"\"\n",
    "    inp = inp.numpy().transpose((1,2,0))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n",
    "        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n",
    "        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n",
    "        return features\n",
    "\n",
    "#Bahdanau Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim) -> None:\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        self.W = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.U = nn.Linear(encoder_dim, attention_dim)\n",
    "\n",
    "        self.A = nn.Linear(attention_dim,1)\n",
    "\n",
    "    def forward(self, features, hidden_state):\n",
    "        u_hs = self.U(features)      #(batch_size,num_layers,attention_dim)\n",
    "        w_ah = self.W(hidden_state)  #(batch_size,attention_dim)\n",
    "\n",
    "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1))  #(batch_size,num_layers,attemtion_dim)\n",
    "\n",
    "        attention_scores = self.A(combined_states)               #(batch_size,num_layers,1)\n",
    "        attention_scores = attention_scores.squeeze(2)           #(batch_size,num_layers)\n",
    "\n",
    "        alpha = F.softmax(attention_scores, dim=1)               #(batch_size,num_layers)\n",
    "\n",
    "        attention_weights = features * alpha.unsqueeze(2)        #(batch_size,num_layers,features_dim)\n",
    "        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n",
    "\n",
    "        return alpha, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, attention_dim, encoder_dim, decoder_dim, drop_prob=0.3) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # save the model params\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, decoder_dim)\n",
    "\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim, decoder_dim, bias=True)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "\n",
    "        self.fcn = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        #Vectorize the caption\n",
    "        embeds = self.embedding(captions)\n",
    "\n",
    "        #Initialise LSTM state\n",
    "        h, c = self.init_hidden_state(features)\n",
    "        #get the seq length to iterate\n",
    "        seq_length = len(captions[0])-1 #Exclude the last one\n",
    "        batch_size = captions.size(0)\n",
    "        num_features = features.size(1)\n",
    "        \n",
    "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n",
    "                \n",
    "        for s in range(seq_length):\n",
    "            alpha,context = self.attention(features, h)\n",
    "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                    \n",
    "            output = self.fcn(self.drop(h))\n",
    "            \n",
    "            preds[:,s] = output\n",
    "            alphas[:,s] = alpha  \n",
    "        \n",
    "        \n",
    "        return preds, alphas\n",
    "    \n",
    "    def generate_caption(self,features,max_len=20,vocab=None):\n",
    "        # Inference part\n",
    "        # Given the image features generate the captions\n",
    "        \n",
    "        batch_size = features.size(0)\n",
    "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        alphas = []\n",
    "        \n",
    "        #starting input\n",
    "        word = torch.tensor(vocab.stoi['<START>']).view(1,-1).to(device)\n",
    "        embeds = self.embedding(word)\n",
    "\n",
    "        \n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            alpha, context = self.attention(features, h)\n",
    "            \n",
    "            #store the apla score\n",
    "            alphas.append(alpha.cpu().detach().numpy())\n",
    "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            output = self.fcn(self.drop(h))\n",
    "            output = output.view(batch_size,-1)\n",
    "        \n",
    "            \n",
    "            #select the word with most val\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            #save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            #end if <EOS detected>\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<END>\":\n",
    "                break\n",
    "            \n",
    "            #send generated word as the next caption\n",
    "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        #covert the vocab idx to words and return sentence\n",
    "        return [vocab.itos[idx] for idx in captions],alphas\n",
    "    \n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN()\n",
    "        self.decoder = DecoderRNN(\n",
    "            embed_size=embed_size,\n",
    "            vocab_size = len(dataset.vocab),\n",
    "            attention_dim=attention_dim,\n",
    "            encoder_dim=encoder_dim,\n",
    "            decoder_dim=decoder_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparams\n",
    "embed_size=300\n",
    "vocab_size = len(dataset.vocab)\n",
    "attention_dim=256\n",
    "encoder_dim=2048\n",
    "decoder_dim=512\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init model\n",
    "model = EncoderDecoder(\n",
    "    embed_size=300,\n",
    "    vocab_size = len(dataset.vocab),\n",
    "    attention_dim=256,\n",
    "    encoder_dim=2048,\n",
    "    decoder_dim=512\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.stoi[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "print_every = 100\n",
    "model.train()\n",
    "for epoch in range(1,num_epochs+1):   \n",
    "    for idx, (image, captions) in enumerate(iter(dataloader)):\n",
    "        image,captions = image.to(device),captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed forward\n",
    "        outputs,attentions = model(image, captions)\n",
    "\n",
    "        # Calculate the batch loss.\n",
    "        targets = captions[:,1:]\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (idx+1)%print_every == 0:\n",
    "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
    "            \n",
    "            #generate the caption\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                dataiter = iter(dataloader)\n",
    "                img,_ = next(dataiter)\n",
    "                features = model.encoder(img[0:1].to(device))\n",
    "                caps, _ = model.decoder.generate_caption(features,vocab=dataset)\n",
    "                caption = ' '.join(caps)\n",
    "                show_image(img[0].cpu(),title=caption)\n",
    "                \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "#Save the model\n",
    "torch.save(model.state_dict(),\"caption_generator.pth\")\n",
    "print(\"Model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
